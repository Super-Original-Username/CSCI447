\documentclass[twoside,11pt]{article}
% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
%\usepackage{utf8}{inputenc}
\usepackage{natbib}



% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\jmlrheading{1}{2019}{1-7}{9/19}{10/9}{meila00a}{Ethan Fison, Zan Montieth, Alex Salois, Sage Acteson}

% Short headings should be running head and authors last names

\ShortHeadings{Simple Validation of the Naïve Bayes Algorithm}{Fison, Montieth, Salois, Acteson}
\firstpageno{1}

\begin{document}


\title{Simple Validation of the Naïve Bayes Algorithm}

\author{\name Ethan Fison \email ethanfison@gmail.com \\
        \name Zan Montieth \email zan.montieth@gmail.com \\
        \name Alex Salois \email asalois.scholar@gmail.com \\ 
        \name Sage Acteson \email sage.acteson@gmail.com \\ 
       \addr Gianforte School of Computing\\
       Montana State University\\
       Bozeman, MT 59715, USA}


\editor{Ethan Fison, Zan Montieth, Alex Salois, Sage Acteson}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file

This paper describes the use of the K Nearest Neighbors algorithm and several of 
its variants for classification and regression of various UCI datasets. These
algorithms are considered lazy learners, as all calculations are performed after
test data is supplied. The general concept behind these algorithms is to find 
the 'distances' between all points in the training set and the input test data, then
find the mean or mode (depending on whether regression or classification are desired)
of the K (K being an experimentally determined hyperparameter closest points, then 
assign that to the class or output value of the test data. Additional methods used include 
Edited K Nearest Neighbors, in which points are removed from the test set if classification
using KNN incorrect. Next was Condensed K Nearest Neighbors, a method in which points are
added to the traning set from zero, if they are incorrectly classified using KNN. The 
clustering methods used were C-Means Clustering and Partitioning Around Medoids.

The results from this experiment were rather surprising, as KNN, despite being less computationally
intense, and not removing noisy data from the training set, was the clear winner in terms of 
overall performance.

\end{abstract}

\begin{keywords}
  Nearest Neighbor, K Nearest Neighbors, Edited K Nearest Neighbors, Condensed Nearest Neighbors, C Means Clustering, Partitioning Around Medoids, Classification, Validation, Regression
\end{keywords}

\section{Introduction}

The goal of this experiment is to compare the efficacy of the K Nearest Neighbors 
algorithm against methods aimed to reduce the size of the training set, using 6 
preselected datasets form the UCI Machine Learning Repository.
\citep{Abalone,Cars,Segmentation,Machine,ForestFires,Wine} To do so, we will employ
ten-fold cross-validation on each applicable algorithm (all algorithms for 
classification, base KNN and both clustering algorithms for regression) using the same
pseudorandomly scrambled version of the dataset to ensure accurate comparisons of performance.
Statistical analysis for this experiment consisted of the use of error, precision, 
recall, and correct classification tally for classification, then mean squared error and 
absolute error for regression.\\


\section{Hypotheses}

For each unmodified set, we assume the null hypothesis. That is, we assume that there will be no 
significant change between the original values and reclassified values for any given test
set sent into our model.\\
For modified sets we expect there to be a measurable increase in the error between the original data and data with scrambled values.

\section{Implementation Decisions}

This section will discuss the design choices made in the development of each major 
python file in the project, in addition to a description of the functionality of 
the associated file(s).

\subsection{File Input}
The file input of our system preprocessed the data in order to optimize it for Naïve Bayes. 
The incoming data was fed into two-dimensional lists then checked for the missing value indicator
listed in the data's corresponding .names file. Then, the data was formatted to the appropriate 
type i.e. integers and floats. Then, a string detecter will parse through the data to find
non numerical data. The parser will then make a list of all unique strings. The unique strings 
are then alphabatized and given integer values to represent them. By alphabatizing the list
beforehand we found that similar vaules in different columns would be assigned the same integer
value making the data much easier to understand. After processing the full data set the two-dimensional
 list was then passed to our ten-fold cross-validation method.  
\subsection{Ten-fold Cross-Validation}

The implementation of the ten-fold cross-validation was intended to be straightforward. 
Due to the nature of this function involving calls to each other python file in our 
project, it was determined that the TenFCV python file would also serve as our driver. 
There are several functions performed within this file. In addition to performing 
ten-fold cross-validation of our data, it supports functionality for scrambling values for 
10\% (rounded up) of rows within a given dataset, scrambling of the overall order of 
rows within the dataset.\\
The first pass of this function for any given dataset will scramble the order of rows, 
split the data set into ten (mostly) equally-sized arrays, then sequentially feed 9 arrays 
into the learner function within the algorithm, followed by feeding the remaining array, 
now with class attributes removed, into the classifier function. The results given by 
the classifer are then fed into our statistical analysis functions along with the original 
segment of the reclassified data for comparison.\\
The next pass will run a dataset with 10\% of its entries changed.\\


\subsection{Statistical Analysis}
For classicfation sets error was calculated using the document given \citep{Precision} from this error 
was calculated using confusion matrixes. Using this method allowed the error, precision, recall to be 
calculceted easily by simply itterating thorugh the confusion matrix and finding the true postives, 
false postives and the false negatives. 0-1 loss which is simply findding the total number correctly and incorrectly 
classified was also used as a metric to easily see if one algorithm variant is more accurate.

For regression mean square error (MSE) and absolute error was used as regression has more of a range than classicfation so it must 
have a more sophisicated way to measure efficacy. MSE is used as measure as it accounts for signs and so all the error will be postive
meaning that a postive and negative error will be acounted for the same. Absolute error will not account for signs but comparing the two
is possible as taking the square root of MSE should give you a value in the ball park of absolute error. 



\section{Results}
Looking at Image Segmentation set and honing in on class one error, one can see that variants of KNN performed worse as the error 0.48 with an 0-1 loss being 
17 correct and 30 incorect for Edited KNN and an astounding error if 0.78 with an 0-1 loss being 9 correct and 75 incorect for Condensed KNN 
comapred to base KNN with error of 0.22 0-1 loss of 24 correct and 14 incorect.  Using this one can see that the hypothesis of there being no change 
between variants of KNN will be rejected.  However looking at PAM and C Means there is not a statistical important differnence from base KNN with error
of 0.22 and 0.25 and 0-1 loss of 24 correct and 14 incorect and 23 correct and 16 incorect respectively. Looking at table 1.1 one can see that this applys
for the overall set.

For Abalone any variant of KNN was a diaster.  The algorithm ran with an eror of nearly one for every variant including PAM and C Means. For this set 
the null hypothesis has failed to be rejected. This set took a long time to run and the algorithm performed poor this points to perhaps that this set 
does not adhere to the requirments of the algorithm. 


PAM and base KNN has the same results Looking further into the Cars set also shows that condensed KNN is the worse of the three KNN variants but 
in this case Edited KNN and base KNN  have nearly the same error showing that condensed KNN will infact reject the null hypothesis while 
Edited KNN does not have a statistical differnce.  

Looking at regression and the ForestFires set with base KNN the MSE is 222328298.0003 while the square root of MSE is 14910.6777
and the absolute error being 334215.1899 which happens to be  the exact same for PAM this error shows that there is not a statistical differnence
between the two.  This is not true for C Means However where the MSE is 222380851.1803 while the square root of MSE is 14912.4394 and the 
absolute error being 334265.1899 showing that there is little to none statistical differnence in any of the algorithm variants.  Therefore 
showing the null hypothesis failed to be rejected. 

Looking at regression and the Machine set with base KNN the MSE is 181545393.0 while the square root of MSE is 13473.8781
and the absolute error being 192235.0 which happens to be the exact same for PAM and C Mean this error shows that there is not a statistical differnence
between the thre.  Therefore showing the null hypothesis failed to be rejected. 




% Acknowledgements should go at the end, before appendices and references

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage





\vskip 0.2in
\bibliography{cite}

\end{document}
