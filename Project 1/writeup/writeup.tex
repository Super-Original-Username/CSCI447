\documentclass[twoside,11pt]{article}
% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
%\usepackage{utf8}{inputenc}
\usepackage{natbib}



% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\jmlrheading{1}{2019}{1-7}{9/19}{9/19}{meila00a}{Ethan Fison, Zan Montieth, Alex Salois, Sage Acteson}

% Short headings should be running head and authors last names

\ShortHeadings{Simple Validation of the Naïve Bayes Algorithm}{Fison, Montieth, Salois, Acteson}
\firstpageno{1}

\begin{document}


\title{Simple Validation of the Naïve Bayes Algorithm}

\author{\name Ethan Fison \email ethanfison@gmail.com \\
        \name Zan Montieth \email zan.montieth@gmail.com \\
        \name Alex Salois \email asalois.scholar@gmail.com \\ 
        \name Sage Acteson \email sage.acteson@gmail.com \\ 
       \addr Gianforte School of Computing\\
       Montana State University\\
       Bozeman, MT 59715, USA}


\editor{Ethan Fison, Zan Montieth, Alex Salois, Sage Acteson}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
This paper describes the Na{\"i}ve Bayes algorithm, used for classification
of data. This algorithm builds its model by finding the average value for each
attribute of a given class, then classifies an input by finding the class it 
most closely matches. In this experiement, we run ten-fold cross-validation 
on our models which are built using 5 different datasets acquired from the UCI Machine 
Learning Repositoryto test the accuracy of the algorithm\\
The algorithm turned out to be very accurate, even when data was scrambled.

\end{abstract}

\begin{keywords}
  Na{\"i}ve Bayes, Classification, Validation
\end{keywords}

\section{Introduction}

The goal of this experiment is to validate the efficacy of the Naïve Bayes 
algorithm. given 5 preselected datasets from the UCI Machine Learning Repository.\citep{Cancer,Glass,Iris,Soybean,Vote}
To do so, we will employ ten-fold cross-validation on each set, with the order of 
entries in the data scrambled prior to being split and passed through Naïve Bayes.
To further examine the capabiities of Naïve Bayes, we will then pass a modified 
version of the original data, this time with the attributes of 10\% (rounded up) of 
classes scrambled within their entries. For this experiment, we have kept our 
statistical analysis simple, building a confusion matrix with the results of each run,
and using that to calculate our error, \citep{Project, Precision} precision, and recall values for the model 
generated in that run.\\



\section{Hypotheses}

For each unmodified set, we assume the null hypothesis. That is, we assume that there will be no 
significant change between the original values and reclassified values for any given test
set sent into our model.\\
For modified sets we expect there to be a measurable increase in the error between the original data and data with scrambled values.

\section{Implementation Decisions}

This section will discuss the design choices made in the development of each major 
python file in the project, in addition to a description of the functionality of 
the associated file(s).

\subsection{File Input}
The file input of our system preprocessed the data in order to optimize it for Naïve Bayes. 
The incoming data was fed into two-dimensional lists then checked for the missing value indicator
listed in the data's corresponding .names file. Then, the data was formatted to the appropriate 
type i.e. integers and floats. Then, a string detecter will parse through the data to find
non numerical data. The parser will then make a list of all unique strings. The unique strings 
are then alphabatized and given integer values to represent them. By alphabatizing the list
beforehand we found that similar vaules in different columns would be assigned the same integer
value making the data much easier to understand. After processing the full data set the two-dimensional
 list was then passed to our ten-fold cross-validation method.  
\subsection{Ten-fold Cross-Validation}

The implementation of the ten-fold cross-validation was intended to be straightforward. 
Due to the nature of this function involving calls to each other python file in our 
project, it was determined that the TenFCV python file would also serve as our driver. 
There are several functions performed within this file. In addition to performing 
ten-fold cross-validation of our data, it supports functionality for scrambling values for 
10\% (rounded up) of rows within a given dataset, scrambling of the overall order of 
rows within the dataset.\\
The first pass of this function for any given dataset will scramble the order of rows, 
split the data set into ten (mostly) equally-sized arrays, then sequentially feed 9 arrays 
into the learner function within the algorithm, followed by feeding the remaining array, 
now with class attributes removed, into the classifier function. The results given by 
the classifer are then fed into our statistical analysis functions along with the original 
segment of the reclassified data for comparison.\\
The next pass will run a dataset with 10\% of its entries changed.

\subsection{Naïve Bayes Implementation}

The Naïve Bayes algorithm was implemented with two major components: train and classify. 
During the training part of the algorithm a two-dimensional list of training data is passed in.
From this data a frequency table is constructed that holds the frequency of each attribute value for each attribute for each class, and the frequency of each class. 
This table used to calculate the probability of each attribute value for each class, and the probability of each individual class.\\
To classify a two-dimensional list of data is passed in, but without the last column, which would normally hold the class.
The probility of each possible class is calculated for each example in the data to be classified, 
and the most likely class is appended to each example (row).
 If all classes have a probability of 0, meaning there were attributes that didn't have a match in any class, then a random class is assigned.
The classified data is then returned for statistical analysis of its accuracy.

\subsection{Statistical Analysis}
Using the document given \citep{Precision} error was calculated using confusion matrixes.
Using this method allowed the error, precision, recall to be calculceted easily by simply
itterating thorugh the confusion matrix and finding the true postives, false postives and the 
false negatives.


\section{Results}
Overall this  alogorithm had little error even when the data was scrambled.  
This was shown by by the Votes data set \citep{Vote}. The alogorithm had an error of 0.19688109, precision of 0.77735849
and reacall of 0.83064516 while when the data was scrambled the alogorithm had an error of 0.28291317, precision 
of 0.75294118 and reacall of 0.68449198.  Looking at the this one can see that the alogorithm is indeed working accurately
and the stats are a worse when part fo the data is scrambled which makes sense and also shows that the data is not just 
random. 


% Acknowledgements should go at the end, before appendices and references

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage





\vskip 0.2in
\bibliography{cite}

\end{document}
